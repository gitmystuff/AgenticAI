{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/AgenticAI/blob/main/06_Exploring_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60cfb2b9",
      "metadata": {
        "id": "60cfb2b9"
      },
      "source": [
        "# Exploring LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d28d9c9",
      "metadata": {
        "id": "8d28d9c9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from anthropic import Anthropic\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0da2c6",
      "metadata": {
        "id": "fe0da2c6"
      },
      "source": [
        "## LLM Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea25618",
      "metadata": {
        "id": "8ea25618",
        "outputId": "38fc831e-252e-4a24-c995-f722cdfbbca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama is running\n",
            "LM Studio is running\n",
            "OpenAI API Key exists\n",
            "Anthropic API Key exists\n",
            "Google API Key exists\n",
            "DeepSeek API Key not set\n",
            "Groq API Key exists\n",
            "Hugging Face Token exists\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(override=True)\n",
        "\n",
        "def is_service_running(url):\n",
        "    \"\"\"\n",
        "    Checks if a service is running by attempting to connect to its URL.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        # Ollama and LM Studio return \"Ollama is running\" or similar on their base URL\n",
        "        # A 200 status code indicates the server is up.\n",
        "        if response.status_code == 200:\n",
        "            return True\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        return False\n",
        "    except requests.exceptions.Timeout:\n",
        "        return False\n",
        "    return False\n",
        "\n",
        "# Check for Ollama\n",
        "ollama_url = 'http://localhost:11434'\n",
        "if is_service_running(ollama_url):\n",
        "    print(\"Ollama is running\")\n",
        "else:\n",
        "    print(\"Ollama is not running\")\n",
        "\n",
        "# Check for LM Studio\n",
        "lmstudio_url = 'http://localhost:1234'\n",
        "if is_service_running(lmstudio_url):\n",
        "    print(\"LM Studio is running\")\n",
        "else:\n",
        "    print(\"LM Studio is not running\")\n",
        "\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
        "groq_api_key = os.getenv('GROQ_API_KEY')\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "\n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key exists\")\n",
        "else:\n",
        "    print(\"Anthropic API Key not set\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists\")\n",
        "else:\n",
        "    print(\"Google API Key not set\")\n",
        "\n",
        "if deepseek_api_key:\n",
        "    print(f\"DeepSeek API Key exists\")\n",
        "else:\n",
        "    print(\"DeepSeek API Key not set\")\n",
        "\n",
        "if groq_api_key:\n",
        "    print(f\"Groq API Key exists\")\n",
        "else:\n",
        "    print(\"Groq API Key not set\")\n",
        "\n",
        "if hf_token:\n",
        "    print(f\"Hugging Face Token exists\")\n",
        "else:\n",
        "    print(\"Hugging Face Token not set\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ea4df3",
      "metadata": {
        "id": "68ea4df3"
      },
      "source": [
        "## Evaluating LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b1471b7",
      "metadata": {
        "id": "2b1471b7",
        "outputId": "387d895b-e52f-446e-88a2-843056e897bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'content': 'You are an AI tasked with writing a single, one-sentence instruction for a human to prevent a paradox in a time-travel scenario. What is that instruction? '}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make a request\n",
        "llms = []\n",
        "responses = []\n",
        "\n",
        "request = \"You are an AI tasked with writing a single, one-sentence instruction for a human to prevent a paradox in a time-travel scenario. What is that instruction? \"\n",
        "messages = [{\"role\": \"user\", \"content\": request}]\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34e1900",
      "metadata": {
        "id": "f34e1900",
        "outputId": "b2b66a18-7d4d-4db9-c482-a068bb263ded"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Do not engage with your past self under any circumstances, as any interaction could lead to unforeseen consequences that may alter your own existence."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# openai\n",
        "openai = OpenAI() # assumes OPENAI_API_KEY is set in environment\n",
        "model = \"gpt-4o-mini\"\n",
        "\n",
        "response = openai.chat.completions.create(model=model, messages=messages)\n",
        "result = response.choices[0].message.content\n",
        "\n",
        "display(Markdown(result))\n",
        "llms.append(model)\n",
        "responses.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1eb75eb",
      "metadata": {
        "id": "a1eb75eb",
        "outputId": "4e0b5ca0-ef81-40d0-90ec-7e7d70c412fe"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Avoid interacting with your past self or altering events you know have already occurred, as this could create a temporal paradox."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# claude\n",
        "model = \"claude-3-7-sonnet-latest\"\n",
        "\n",
        "claude = Anthropic()\n",
        "response = claude.messages.create(model=model, messages=messages, max_tokens=1000)\n",
        "result = response.content[0].text\n",
        "\n",
        "display(Markdown(result))\n",
        "llms.append(model)\n",
        "responses.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6d5757",
      "metadata": {
        "id": "df6d5757",
        "outputId": "6a13a726-a4e7-49d4-e0c8-9e8b0bd1cd38"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Do not interact in any way that would prevent your own birth or your decision to time travel.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# gemini, may need enable Generative AI API in Google Cloud Console\n",
        "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
        "model = \"gemini-2.0-flash\"\n",
        "\n",
        "response = gemini.chat.completions.create(model=model, messages=messages)\n",
        "result = response.choices[0].message.content\n",
        "\n",
        "display(Markdown(result))\n",
        "llms.append(model)\n",
        "responses.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4878e5d8",
      "metadata": {
        "id": "4878e5d8",
        "outputId": "fb882a7a-edac-4fd6-b8e1-0a070e7b1f71"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "If you are about to interact with your past self or any event that has already occurred, do not give or receive any information that would cause your past self or anyone else to take an action that you have already witnessed or know to be a part of established events."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# groq\n",
        "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
        "model = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "response = groq.chat.completions.create(model=model, messages=messages)\n",
        "result = response.choices[0].message.content\n",
        "\n",
        "display(Markdown(result))\n",
        "llms.append(model)\n",
        "responses.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d741df6",
      "metadata": {
        "id": "8d741df6",
        "outputId": "71c97353-f570-492d-8293-6dbb2ee35c06"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To avoid a paradox arising from interacting with your earlier time-self while on a journey through the timestream, refrain from altering any events or taking actions that could have been the immediate cause of initiating this same trip."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
        "model = \"llama3.2\"\n",
        "\n",
        "response = ollama.chat.completions.create(model=model, messages=messages)\n",
        "result = response.choices[0].message.content\n",
        "\n",
        "display(Markdown(result))\n",
        "llms.append(model)\n",
        "responses.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bab68145",
      "metadata": {
        "id": "bab68145",
        "outputId": "2012d03a-8f08-4906-eef9-827a233fc799"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " Do not create an event in the past that would alter the circumstances leading to your presence in the current moment in time."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mistral = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
        "model = \"mistral_instruct_7b\"\n",
        "\n",
        "response = mistral.chat.completions.create(model=model, messages=messages)\n",
        "result = response.choices[0].message.content\n",
        "\n",
        "display(Markdown(result))\n",
        "llms.append(model)\n",
        "responses.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b619e9b1",
      "metadata": {
        "id": "b619e9b1"
      },
      "outputs": [],
      "source": [
        "text_prep = \"\"\n",
        "\n",
        "for index, response in enumerate(responses):\n",
        "    text_prep += f\"# Response from llm {index+1}\\n\\n\"\n",
        "    text_prep += response + \"\\n\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c601083",
      "metadata": {
        "id": "8c601083"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a34705e",
      "metadata": {
        "id": "2a34705e",
        "outputId": "85e92fdd-989b-4fce-b322-c443928ec15c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are evaluating responses from 6 LLMs.\n",
            "Each model has been given this question:\n",
            "\n",
            "You are an AI tasked with writing a single, one-sentence instruction for a human to prevent a paradox in a time-travel scenario. What is that instruction? \n",
            "\n",
            "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
            "Respond with JSON, and only JSON, with the following format:\n",
            "{\"results\": [\"best llm number\", \"second best llm number\", \"third best llm number\", ...]}\n",
            "\n",
            "Here are the responses from each llm:\n",
            "\n",
            "# Response from llm 1\n",
            "\n",
            "Do not engage with your past self under any circumstances, as any interaction could lead to unforeseen consequences that may alter your own existence.\n",
            "\n",
            "# Response from llm 2\n",
            "\n",
            "Avoid interacting with your past self or altering events you know have already occurred, as this could create a temporal paradox.\n",
            "\n",
            "# Response from llm 3\n",
            "\n",
            "Do not interact in any way that would prevent your own birth or your decision to time travel.\n",
            "\n",
            "\n",
            "# Response from llm 4\n",
            "\n",
            "If you are about to interact with your past self or any event that has already occurred, do not give or receive any information that would cause your past self or anyone else to take an action that you have already witnessed or know to be a part of established events.\n",
            "\n",
            "# Response from llm 5\n",
            "\n",
            "To avoid a paradox arising from interacting with your earlier time-self while on a journey through the timestream, refrain from altering any events or taking actions that could have been the immediate cause of initiating this same trip.\n",
            "\n",
            "# Response from llm 6\n",
            "\n",
            " Do not create an event in the past that would alter the circumstances leading to your presence in the current moment in time.\n",
            "\n",
            "\n",
            "\n",
            "Now, please respond with the ranked order of the llms using JSON, nothing else. Do not include markdown formatting or code blocks.\n"
          ]
        }
      ],
      "source": [
        "evaluator = f\"\"\"You are evaluating responses from {len(llms)} LLMs.\n",
        "Each model has been given this question:\n",
        "\n",
        "{request}\n",
        "\n",
        "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
        "Respond with JSON, and only JSON, with the following format:\n",
        "{{\"results\": [\"best llm number\", \"second best llm number\", \"third best llm number\", ...]}}\n",
        "\n",
        "Here are the responses from each llm:\n",
        "\n",
        "{text_prep}\n",
        "\n",
        "Now, please respond with the ranked order of the llms using JSON, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n",
        "\n",
        "print(evaluator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ce18f30",
      "metadata": {
        "id": "2ce18f30",
        "outputId": "1d76e5d4-777d-42b5-b96a-664ae0ada0f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"results\": [\"1\", \"2\", \"6\", \"4\", \"5\", \"3\"]}\n"
          ]
        }
      ],
      "source": [
        "evaluator_messages = [{\"role\": \"user\", \"content\": evaluator}]\n",
        "\n",
        "openai = OpenAI()\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"o3-mini\",\n",
        "    messages=evaluator_messages,\n",
        ")\n",
        "results = response.choices[0].message.content\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bfa3883",
      "metadata": {
        "id": "2bfa3883",
        "outputId": "d8aa7450-12a4-43aa-a661-5251e6b7a69e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 1: gpt-4o-mini\n",
            "Rank 2: claude-3-7-sonnet-latest\n",
            "Rank 3: mistral_instruct_7b\n",
            "Rank 4: llama-3.3-70b-versatile\n",
            "Rank 5: llama3.2\n",
            "Rank 6: gemini-2.0-flash\n"
          ]
        }
      ],
      "source": [
        "results_dict = json.loads(results)\n",
        "ranks = results_dict[\"results\"]\n",
        "for index, result in enumerate(ranks):\n",
        "    llm = llms[int(result)-1]\n",
        "    print(f\"Rank {index+1}: {llm}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agents-playground",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}