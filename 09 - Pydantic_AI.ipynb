{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/AgenticAI/blob/main/09_Pydantic_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvJwJKyaS-Mz"
      },
      "source": [
        "# Pydantic AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abDGEhO7s56U"
      },
      "outputs": [],
      "source": [
        "# pip install pydantic-ai logfire nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f967cbee-b5fa-4ba1-8f12-73753d8b01ea"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from pydantic import BaseModel\n",
        "from pydantic_ai import Agent, ModelRetry, RunContext\n",
        "from pydantic_ai.models.groq import GroqModel\n",
        "from dotenv import load_dotenv\n",
        "# from google.colab import userdata\n",
        "import nest_asyncio\n",
        "import logfire\n",
        "logfire.configure(send_to_logfire='if-token-present')\n",
        "\n",
        "load_dotenv()\n",
        "# api_key = userdata.get('GROQ_API_KEY')\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF_H-ICU0UGi",
        "outputId": "e9abc48e-bb9a-4113-f6d8-b1b338f40b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AgentRunResult(output='Hello. How can I assist you today?')\n"
          ]
        }
      ],
      "source": [
        "model = GroqModel('llama-3.3-70b-versatile')\n",
        "agent = Agent(model)\n",
        "result = agent.run_sync(\"Say Hello\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cd91b06-4440-4362-9af0-2300e5ffdf44",
        "outputId": "d24d9d58-964d-410d-af9a-e3a7706d3514"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello. How can I assist you today?'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--kRB-6qd_KM",
        "outputId": "3bb99196-cd57-4629-c2f2-919a850fcedb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Agent(model=GroqModel(), name='agent', end_strategy='early', model_settings=None, output_type=<class 'str'>, instrument=None)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWAJnDJbV3nq",
        "outputId": "aedd64d3-8015-4cf5-cf9d-24ba0f6e3ac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"Here's a joke for you:\\n\"\n",
            " '\\n'\n",
            " 'What do you call a fake noodle?\\n'\n",
            " '\\n'\n",
            " 'An impasta!\\n'\n",
            " '\\n'\n",
            " 'I hope that made you smile! Do you want to hear another one?')\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "agent = Agent(model, system_prompt='Be a helpful assistant.')\n",
        "result = agent.run_sync('Tell me a joke.')\n",
        "pprint.pprint(result.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGZ9ejZfWheL",
        "outputId": "34bad3da-d4be-42dc-e398-cbc7ddf67f73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AgentRunResult(output='The joke is a play on words. \"Impasta\" sounds similar to \"impostor\", which means something that is fake or pretending to be something else. But in this case, \"impasta\" is also a pun on the word \"pasta\", which is a type of noodle.\\n\\nSo, the joke is saying that a fake noodle (something that is pretending to be a noodle but isn\\'t really one) is called an \"impasta\" - it\\'s a fake pasta, or an impostor noodle. It\\'s a lighthearted and silly joke, and the wordplay is what makes it funny!')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result2 = agent.run_sync('Explain it please', message_history=result.new_messages())\n",
        "result2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNxF_fGQXBSd",
        "outputId": "9333c9c5-ab82-4704-9cc0-671f6d9d6224"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.', timestamp=datetime.datetime(2025, 10, 19, 15, 55, 9, 845526, tzinfo=datetime.timezone.utc)), UserPromptPart(content='Tell me a joke.', timestamp=datetime.datetime(2025, 10, 19, 15, 55, 9, 845526, tzinfo=datetime.timezone.utc))]),\n",
              " ModelResponse(parts=[TextPart(content=\"Here's a joke for you:\\n\\nWhat do you call a fake noodle?\\n\\nAn impasta!\\n\\nI hope that made you smile! Do you want to hear another one?\")], usage=RequestUsage(input_tokens=45, output_tokens=36), model_name='llama-3.3-70b-versatile', timestamp=datetime.datetime(2025, 10, 19, 15, 55, 11, tzinfo=TzInfo(UTC)), provider_name='groq', provider_response_id='chatcmpl-a49598d6-e43f-4379-b926-10f508b0f189'),\n",
              " ModelRequest(parts=[UserPromptPart(content='Explain it please', timestamp=datetime.datetime(2025, 10, 19, 15, 55, 24, 667291, tzinfo=datetime.timezone.utc))]),\n",
              " ModelResponse(parts=[TextPart(content='The joke is a play on words. \"Impasta\" sounds similar to \"impostor\", which means something that is fake or pretending to be something else. But in this case, \"impasta\" is also a pun on the word \"pasta\", which is a type of noodle.\\n\\nSo, the joke is saying that a fake noodle (something that is pretending to be a noodle but isn\\'t really one) is called an \"impasta\" - it\\'s a fake pasta, or an impostor noodle. It\\'s a lighthearted and silly joke, and the wordplay is what makes it funny!')], usage=RequestUsage(input_tokens=94, output_tokens=132), model_name='llama-3.3-70b-versatile', timestamp=datetime.datetime(2025, 10, 19, 15, 55, 26, tzinfo=TzInfo(UTC)), provider_name='groq', provider_response_id='chatcmpl-e905be99-8b26-4d51-b226-c950bfa5b3cf')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result2.all_messages()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeMOfH3CXFLe",
        "outputId": "8d83a694-e29c-4cfb-e1d5-cc5413536415"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ModelRequest(parts=[UserPromptPart(content='Explain it please', timestamp=datetime.datetime(2025, 10, 19, 15, 55, 24, 667291, tzinfo=datetime.timezone.utc))]),\n",
              " ModelResponse(parts=[TextPart(content='The joke is a play on words. \"Impasta\" sounds similar to \"impostor\", which means something that is fake or pretending to be something else. But in this case, \"impasta\" is also a pun on the word \"pasta\", which is a type of noodle.\\n\\nSo, the joke is saying that a fake noodle (something that is pretending to be a noodle but isn\\'t really one) is called an \"impasta\" - it\\'s a fake pasta, or an impostor noodle. It\\'s a lighthearted and silly joke, and the wordplay is what makes it funny!')], usage=RequestUsage(input_tokens=94, output_tokens=132), model_name='llama-3.3-70b-versatile', timestamp=datetime.datetime(2025, 10, 19, 15, 55, 26, tzinfo=TzInfo(UTC)), provider_name='groq', provider_response_id='chatcmpl-e905be99-8b26-4d51-b226-c950bfa5b3cf')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result2.new_messages()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_okR_StxY0T3",
        "outputId": "4afd0f73-c440-42d1-ff52-4371b5a60330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='Thomas Edison' era='19th-20th century' primary_contribution='invention of the first commercially practical incandescent light bulb'\n"
          ]
        }
      ],
      "source": [
        "class HistoricalFigureModel(BaseModel):\n",
        "    name: str\n",
        "    era: str\n",
        "    primary_contribution: str\n",
        "    # famous_quote: str\n",
        "\n",
        "agent = Agent(\n",
        "    model,\n",
        "    output_type=HistoricalFigureModel\n",
        ")\n",
        "\n",
        "result = agent.run_sync('Summarize the inventor of the lightbulb')\n",
        "print(result.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlGSSpk43ffH",
        "outputId": "6d4ba617-9a8b-414f-ea8d-a2df4f3ebe52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='Leonardo da Vinci' era='Renaissance' primary_contribution='Mona Lisa painting' famous_quote='The noblest pleasure is the joy of understanding'\n"
          ]
        }
      ],
      "source": [
        "class HistoricalFigureModel(BaseModel):\n",
        "    name: str\n",
        "    era: str\n",
        "    primary_contribution: str\n",
        "    famous_quote: str\n",
        "\n",
        "agent = Agent(\n",
        "    model,\n",
        "    output_type=HistoricalFigureModel\n",
        ")\n",
        "\n",
        "result = agent.run_sync('Tell me about the painter of the Mona Lisa')\n",
        "print(result.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtdBEuoTaWlL"
      },
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v24W5u49JdVY",
        "outputId": "c9b044f9-4305-4bdb-aab1-e35b3e6a7d01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "task='create slides' status='in progress' last_task='create slides'\n",
            "task='review slides' status='completed' last_task='review slides'\n",
            "task='Task 1' status='completed' last_task='Task 1'\n",
            "task='review slides' status='done' last_task='review slides'\n"
          ]
        }
      ],
      "source": [
        "from typing import Optional\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MyModel(BaseModel):\n",
        "    task: str\n",
        "    status: str\n",
        "    last_task: Optional[str]\n",
        "\n",
        "agent = Agent(\n",
        "    model=model,  # Or your small local model\n",
        "    output_type=MyModel,\n",
        "    system_prompt=\"Keep responses short. Handle one task at a time.\",\n",
        "    retries=3\n",
        ")\n",
        "\n",
        "last_task = None\n",
        "last_status = None\n",
        "\n",
        "@agent.tool\n",
        "async def add_task(ctx: RunContext, description: str) -> MyModel:\n",
        "    \"\"\"Adds a new task.\"\"\"\n",
        "    global last_task, last_status\n",
        "    last_task = description\n",
        "    last_status = \"pending\"\n",
        "    return MyModel(task=description, status=\"pending\", last_task=None)\n",
        "\n",
        "@agent.tool\n",
        "async def complete_last(ctx: RunContext) -> MyModel:\n",
        "    \"\"\"Marks the last task as completed.\"\"\"\n",
        "    global last_task, last_status\n",
        "    if not last_task:\n",
        "        raise ValueError(\"No previous task found.\")\n",
        "    last_status = \"done\"\n",
        "    return MyModel(task=last_task, status=\"done\", last_task=last_task)\n",
        "\n",
        "result = agent.run_sync(\"Add a task: create slides.\")\n",
        "print(result.output)\n",
        "\n",
        "result = agent.run_sync(\"Add a task: review slides.\")\n",
        "print(result.output)\n",
        "\n",
        "result = agent.run_sync(\"Now mark the first task as done.\")\n",
        "print(result.output)\n",
        "\n",
        "result = agent.run_sync(\"Now mark the last task as done.\")\n",
        "print(result.output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "agents-playground",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
